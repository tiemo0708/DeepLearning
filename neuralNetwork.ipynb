{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n",
      "(3,)\n",
      "(3, 2)\n",
      "(2,)\n",
      "[0.51615984 1.21402696]\n",
      "[0.62624937 0.7710107 ]\n",
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "#3.4.2 각층의 신호 전달 구현하기\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "\n",
    "#입력층에서 1층으로의 신호 전달\n",
    "X = np.array([1.0, 0.5]) #입력층 뉴런\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) #1층의 가중치\n",
    "B1 = np.array([0.1, 0.2, 0.3]) #1층의 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W1.shape)\n",
    "print(B1.shape)\n",
    "A1 = np.dot(X, W1) + B1 # 1층 뉴런들의 값\n",
    "Z1 = sigmoid(A1) #활성화 함수로 변환된 신호\n",
    "\n",
    "print(A1)\n",
    "print(Z1)\n",
    "\n",
    "#1층에서 2층으로의 신호 전달\n",
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) #2층의 가중치\n",
    "B2 = np.array([0.1, 0.2]) #2층의 편향\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2 = np.dot(Z1, W2) + B2 # 2층 뉴런들의 값\n",
    "Z2 = sigmoid(A2) #활성화 함수로 변환된 신호\n",
    "\n",
    "print(A2)\n",
    "print(Z2)\n",
    "\n",
    "#2층에서 출력층으로의 신호 전달\n",
    "def identify_function(x):\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]]) #출력층의 가중치\n",
    "B3 = np.array([0.1, 0.2]) #출력층의 편향\n",
    "\n",
    "A3 = np.dot(Z2, W3) + B3 # 출력층 뉴런들의 값\n",
    "Y = identify_function(A3) #활성화 함수로 변환된 신호\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "#3.4.3 구현정리\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def identify_function(x):\n",
    "    return x\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identify_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34985881 18.17414537 54.59815003]\n",
      "74.1221542101633\n",
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "#3.5.1 항등함수와 소프트맥스 함수 구현하기\n",
    "#소프트맥스 함수 구현\n",
    "\n",
    "a = np.array([0.3, 2.9, 4.0])\n",
    "\n",
    "exp_a = np.exp(a) #지수함수\n",
    "print(exp_a)\n",
    "\n",
    "sum_exp_a = sum(exp_a) #지수함수의 합\n",
    "print(sum_exp_a)\n",
    "\n",
    "y = exp_a/sum_exp_a\n",
    "print(y)\n",
    "\n",
    "#함수로 구현\n",
    "def softmax(a):\n",
    "    exp_a =np.exp(a)\n",
    "    sum_exp_a = sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan]\n",
      "[  0 -10 -20]\n",
      "[9.99954600e-01 4.53978686e-05 2.06106005e-09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#3.5.2 소프트맥스 함수 구현시 주의점\n",
    "#소프트맥스의 지수 함수를 계산할때 어떤 정수를 더해도 결과는 바뀌지 않는다는 점을 이용\n",
    "#오버 플로를 막을 목적으로 입력 신호중 최댓값 이용\n",
    "\n",
    "a = np.array([1010, 1000, 990])\n",
    "print(np.exp(a)/np.sum(np.exp(a))) #제대로 계산X\n",
    "c = np.max(a) # c = 1010 (최댓값)\n",
    "print(a-c)\n",
    "\n",
    "print(np.exp(a-c)/np.sum(np.exp(a-c))) # 최댓값을 빼주면 제대로 계산가능\n",
    "\n",
    "# 오버플로 대책으로 다시구현\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a =np.exp(a-c) #오버플로 대책\n",
    "    sum_exp_a = sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "\n",
    "    return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
